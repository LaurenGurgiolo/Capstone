{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca2cd7b0-6364-4866-92c6-0cfa0792b36e",
   "metadata": {
    "gather": {
     "logged": 1765251056045
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'label'],\n",
       "    num_rows: 91\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, concatenate_datasets, Audio\n",
    "\n",
    "arrow_files = [\"SoundCloud_1000/data-00000-of-00011.arrow\", \"SoundCloud_1000/data-00001-of-00011.arrow\", 'SoundCloud_1000/data-00002-of-00011.arrow', \"SoundCloud_1000/data-00003-of-00011.arrow\", \"SoundCloud_1000/data-00004-of-00011.arrow\", \"SoundCloud_1000/data-00005-of-00011.arrow\", \"SoundCloud_1000/data-00006-of-00011.arrow\",\"SoundCloud_1000/data-00007-of-00011.arrow\",\"SoundCloud_1000/data-00008-of-00011.arrow\", \"SoundCloud_1000/data-00009-of-00011.arrow\", \"SoundCloud_1000/data-00010-of-00011.arrow\"]\n",
    "\n",
    "# Load each Arrow file into a separate Dataset object\n",
    "individual_datasets = [Dataset.from_file(arrow_file) for arrow_file in arrow_files]\n",
    "processed_dataset = concatenate_datasets(individual_datasets)\n",
    "processed_dataset = processed_dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "new_datasets = []\n",
    "for x in range(len(individual_datasets)):\n",
    "    custom_dataset_x = individual_datasets[x].cast_column(\"audio\", Audio(decode=False))\n",
    "    new_datasets.append(custom_dataset_x)\n",
    "\n",
    "# Concatenate the individual datasets\n",
    "\n",
    "\n",
    "TARGET_SAMPLING_RATE = 16000\n",
    "new_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c991af6-bb87-4438-b46b-1be79c4c8786",
   "metadata": {
    "gather": {
     "logged": 1765251056119
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from datasets import Dataset, Audio\n",
    "import numpy as np\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader\n",
    "import io\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor, TrainingArguments, Trainer, AutoConfig \n",
    "\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    processed_audio_arrays = []\n",
    "    labels = []\n",
    "    model_checkpoint = \"fine-tuned-model2\"\n",
    "    config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # --- KEY FIX ---\n",
    "    # 1. Check if the 'num_channels' attribute exists and remove it if it does\n",
    "    # This prevents the TypeError\n",
    "    if hasattr(config, 'num_channels'):\n",
    "        print(f\"Removing problematic 'num_channels' attribute from config: {config.num_channels}\")\n",
    "        del config.num_channels\n",
    "    # ---------------\n",
    "    \n",
    "    # 2. Modify the config for your specific task (e.g., number of labels)\n",
    "    num_labels = 11\n",
    "    config.num_labels = num_labels\n",
    "    \n",
    "    # 3. Load the model using the modified configuration\n",
    "    model = AutoModelForAudioClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        config=config,  # Pass the modified config object\n",
    "        ignore_mismatched_sizes=True # Set to True to handle new num_labels\n",
    "    )\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "\n",
    "    audio_entry = examples['audio']\n",
    "    label = examples['label'] # Column name may vary by dataset, adjust as needed\n",
    "\n",
    "    try:\n",
    "        if \"bytes\" in audio_entry:\n",
    "            audio_bytes = audio_entry[\"bytes\"]\n",
    "            # Decode using librosa\n",
    "            audio, sr = librosa.load(io.BytesIO(audio_bytes), sr=16000, mono=True)\n",
    "            print(f\"Decoded audio array shape: {audio.shape}, sample rate: {sr}\")\n",
    "            expected_length = TARGET_SAMPLING_RATE * 30\n",
    "            if len(audio) >= expected_length:\n",
    "                # Truncate\n",
    "                audio = audio[:expected_length]\n",
    "            else:\n",
    "                # Pad with zeros\n",
    "                audio = np.pad(audio, (0, expected_length - len(audio)), 'constant')\n",
    "\n",
    "                \n",
    "       \n",
    "        inputs = feature_extractor(audio, sampling_rate=feature_extractor.sampling_rate)\n",
    "        inputs[\"input_values\"] = torch.tensor(inputs[\"input_values\"]).squeeze(0)   \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {audio}: {e}\")\n",
    "    \n",
    "        \n",
    "    # Return a dictionary with the new processed arrays and labels\n",
    "    \n",
    "\n",
    "            \n",
    "    # Return a dictionary with the new processed arrays and labels\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "536f8b7c-49fd-4d2b-b09d-8eb43e718d55",
   "metadata": {
    "gather": {
     "logged": 1765251175510
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340dbc1e522d470a838f32cea2fe47d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d551c1eb19694b86a6f698535b0c3262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b9fe6375a04db4bbcefed2f8cd8aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04fedbec2ef4f8d8969be07574c39d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3a740dceb8416cad1a3b6a19bf0fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdd85bae2294810abeb1405ced4aa4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29e72615e42408080663f606afac505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eaf8ca9bd7b4520b6d72ccba57cca3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2c9db93f4345aaa685a53efd48f4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef43d0572b641628599fa010599d9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928965fb3b634eb788784c4cd206a244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'label', 'input_values'],\n",
      "    num_rows: 91\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['audio', 'label', 'input_values'],\n",
       "     num_rows: 91\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['audio', 'label', 'input_values'],\n",
       "     num_rows: 91\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['audio', 'label', 'input_values'],\n",
       "     num_rows: 91\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['audio', 'label', 'input_values'],\n",
       "     num_rows: 91\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['audio', 'label', 'input_values'],\n",
       "     num_rows: 91\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['audio', 'label', 'input_values'],\n",
       "     num_rows: 91\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['audio', 'label', 'input_values'],\n",
       "     num_rows: 91\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['audio', 'label', 'input_values'],\n",
       "     num_rows: 91\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['audio', 'label', 'input_values'],\n",
       "     num_rows: 91\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['audio', 'label', 'input_values'],\n",
       "     num_rows: 91\n",
       " })]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from datasets import Dataset, Audio\n",
    "import numpy as np\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader\n",
    "import io\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "\n",
    "proccessed_new_dataset = []\n",
    "\n",
    "processed_dataset1 = new_datasets[0].map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "     # Only load \"audio\" for the map operation\n",
    ")\n",
    "proccessed_new_dataset.append(processed_dataset1) \n",
    "processed_dataset1.save_to_disk(\"processed_dataset1\")\n",
    "\n",
    "processed_dataset2 = new_datasets[1].map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "    \n",
    "     # Only load \"audio\" for the map operation\n",
    ")\n",
    "proccessed_new_dataset.append(processed_dataset2) \n",
    "processed_dataset2.save_to_disk(\"processed_dataset2\")\n",
    "\n",
    "processed_dataset3 = new_datasets[2].map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "    \n",
    "     # Only load \"audio\" for the map operation\n",
    ")\n",
    "proccessed_new_dataset.append(processed_dataset3)\n",
    "processed_dataset3.save_to_disk(\"processed_dataset3\")\n",
    "\n",
    "processed_dataset4 = new_datasets[3].map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "    \n",
    "     # Only load \"audio\" for the map operation\n",
    ")\n",
    "proccessed_new_dataset.append(processed_dataset4)\n",
    "processed_dataset4.save_to_disk(\"processed_dataset4\")\n",
    "\n",
    "processed_dataset5 = new_datasets[4].map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "   \n",
    "     # Only load \"audio\" for the map operation\n",
    ")\n",
    "proccessed_new_dataset.append(processed_dataset5)\n",
    "processed_dataset5.save_to_disk(\"processed_dataset5\")\n",
    "\n",
    "processed_dataset6 = new_datasets[5].map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "    \n",
    "     # Only load \"audio\" for the map operation\n",
    ")\n",
    "proccessed_new_dataset.append(processed_dataset5)\n",
    "processed_dataset6.save_to_disk(\"processed_dataset6\")\n",
    "\n",
    "\n",
    "processed_dataset7 = new_datasets[6].map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "    \n",
    "     # Only load \"audio\" for the map operation\n",
    ")\n",
    "proccessed_new_dataset.append(processed_dataset7) \n",
    "processed_dataset7.save_to_disk(\"processed_dataset7\")\n",
    "\n",
    "processed_dataset8 = new_datasets[7].map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "    \n",
    "     # Only load \"audio\" for the map operation\n",
    ")\n",
    "proccessed_new_dataset.append(processed_dataset8) \n",
    "processed_dataset8.save_to_disk(\"processed_dataset8\")\n",
    "\n",
    "processed_dataset9 = new_datasets[8].map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "    \n",
    "     # Only load \"audio\" for the map operation\n",
    ")\n",
    "proccessed_new_dataset.append(processed_dataset9)\n",
    "processed_dataset9.save_to_disk(\"processed_dataset9\")\n",
    "\n",
    "processed_dataset10 = new_datasets[9].map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "   \n",
    "     # Only load \"audio\" for the map operation\n",
    ")\n",
    "proccessed_new_dataset.append(processed_dataset10)\n",
    "processed_dataset10.save_to_disk(\"processed_dataset10\")\n",
    "\n",
    "eval_dataset = new_datasets[10].map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "   \n",
    "     # Only load \"audio\" for the map operation\n",
    ")\n",
    "\n",
    "eval_dataset.save_to_disk(\"eval_dataset\")\n",
    "print(eval_dataset)\n",
    "proccessed_new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7f9d85b-96d6-4c56-aee3-9597bcb17f60",
   "metadata": {
    "gather": {
     "logged": 1765251175568
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = concatenate_datasets(proccessed_new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52bb3411-043e-4744-9674-42f14a53ea85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'label'],\n",
       "    num_rows: 1001\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1929bc9d-f1b5-4d24-a644-f54d55e53392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fff9e82-9899-418f-8872-58a6414e02a1",
   "metadata": {
    "gather": {
     "logged": 1765283742146
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's target sampling rate is: 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [570/570 4:25:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "0.0284\n",
      "Attempted to log scalar metric grad_norm:\n",
      "0.034337256103754044\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.4912280701754387e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "4.385964912280702\n",
      "Attempted to log scalar metric train_runtime:\n",
      "15960.2235\n",
      "Attempted to log scalar metric train_samples_per_second:\n",
      "0.285\n",
      "Attempted to log scalar metric train_steps_per_second:\n",
      "0.036\n",
      "Attempted to log scalar metric total_flos:\n",
      "3.084366866939904e+17\n",
      "Attempted to log scalar metric train_loss:\n",
      "0.02638406157493591\n",
      "Attempted to log scalar metric epoch:\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    " # 4. Main training loop\n",
    "\n",
    "    # --- Dummy Data Generation (Replace with your actual data loading) ---\n",
    "sampling_rate = 16000\n",
    "num_samples = 1001\n",
    "model_checkpoint = \"fine-tuned-model2\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "\n",
    "# --- KEY FIX ---\n",
    "# 1. Check if the 'num_channels' attribute exists and remove it if it does\n",
    "# This prevents the TypeError\n",
    "if hasattr(config, 'num_channels'):\n",
    "    print(f\"Removing problematic 'num_channels' attribute from config: {config.num_channels}\")\n",
    "    del config.num_channels\n",
    "# ---------------\n",
    "\n",
    "# 2. Modify the config for your specific task (e.g., number of labels)\n",
    "num_labels = 11\n",
    "config.num_labels = num_labels\n",
    "\n",
    "# 3. Load the model using the modified configuration\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    config=config,  # Pass the modified config object\n",
    "    ignore_mismatched_sizes=True # Set to True to handle new num_labels\n",
    ")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Ensure all tensors and models are moved to this device\n",
    "model.to(device)\n",
    "\n",
    "# Apply the function across the dataset\n",
    "\n",
    "# Split into train and test (optional, but good practice)\n",
    "\n",
    "\n",
    "labels = ['Angry', 'Contempt', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Sleepy', 'Surprised', 'Bad', 'Boring']   \n",
    "\n",
    "id2label = {i: name for i, name in enumerate(labels)}\n",
    "\n",
    "\n",
    "\n",
    "label2id = {name: i for i, name in enumerate(labels)}\n",
    "\n",
    "\n",
    "# Define number of classes\n",
    "num_classes = len(set(labels))\n",
    "\n",
    "\n",
    "# Set the target sampling rate for the feature extractor.\n",
    "target_sampling_rate = 16000\n",
    "print(f\"The model's target sampling rate is: {target_sampling_rate}\")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned-model3\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs', # Added logging directory for better tracking\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./fine-tuned-model3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f859bbeb-3052-4c30-b109-457bb1da1b8e",
   "metadata": {
    "gather": {
     "logged": 1765283744324
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./fine-tuned-model3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c880d12-0dbf-4b9e-b785-06365dd81c03",
   "metadata": {
    "gather": {
     "logged": 1765283838978
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "3.0488290786743164\n",
      "Attempted to log scalar metric eval_model_preparation_time:\n",
      "0.0026\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.3956043956043956\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "93.5191\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "0.973\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "0.128\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from datasets import Dataset, Audio, ClassLabel, Features \n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer \n",
    "from huggingface_hub import notebook_login \n",
    "import evaluate \n",
    "import torch\n",
    "import librosa \n",
    "from datasets import Dataset, Audio \n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor, TrainingArguments, Trainer, AutoConfig \n",
    "from accelerate import Accelerator\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "print(eval_dataset)\n",
    "model_checkpoint = \"./fine-tuned-model3\"\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "      # Pass the modified config object\n",
    "    ignore_mismatched_sizes=True # Set to True to handle new num_labels\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    eval_dataset=eval_dataset,\n",
    "compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "224d86ae-3dbb-4ef6-b5b2-2f4623893b37",
   "metadata": {
    "gather": {
     "logged": 1765283839802
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0488290786743164, 'eval_model_preparation_time': 0.0026, 'eval_accuracy': 0.3956043956043956, 'eval_runtime': 93.5191, 'eval_samples_per_second': 0.973, 'eval_steps_per_second': 0.128}\n"
     ]
    }
   ],
   "source": [
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2ff759f-7a93-4073-a3be-91110380944c",
   "metadata": {
    "gather": {
     "logged": 1765283932714
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: [ 6  4  4  4  6  4  6  6  5  1  5  4  2  6  4  1  5  2  4  7  9  6  6  4\n",
      "  1  8  6  1  7  6  5  8  7 10  5  1  6  5  9 10  5 10  4  6  1  1  1  4\n",
      "  5  4  7  5  6  6  0  5  5 10  4  5  8  9 10  6  1  6  7  9  1  6  7  6\n",
      "  0  9  4  7  6 10  5  6  2  2  4 10  5  1  5  4  9  6  9]\n",
      "True Labels: [ 5  4  6  1  7  6  1  1  9  5  9  5  2  1  4  1  9  7  9  7  1  6  8  0\n",
      "  9  8  6  1  9  7  9  8  6  9  9  2  5  6  6  2  9 10  4  6  7  8  1  9\n",
      "  4  8  6  9  6  6  1  1  5 10  4  1  8  4  0  6  5  8  7  9  5  5  7  6\n",
      "  0  9  4  6  6 10  1  1  2  9  1 10  7  1  5  1  1  6  9]\n",
      "Evaluation Metrics: {'test_loss': 3.0488290786743164, 'test_model_preparation_time': 0.0026, 'test_accuracy': 0.3956043956043956, 'test_runtime': 93.7643, 'test_samples_per_second': 0.971, 'test_steps_per_second': 0.128}\n"
     ]
    }
   ],
   "source": [
    "predictions_output = trainer.predict(eval_dataset)\n",
    "\n",
    "# The predictions_output object contains various information:\n",
    "# predictions_output.predictions: Raw model outputs (logits)\n",
    "# predictions_output.label_ids: True labels from the eval_dataset\n",
    "# predictions_output.metrics: Evaluation metrics (e.g., accuracy)\n",
    "\n",
    "# To get the predicted class labels:\n",
    "predicted_labels = np.argmax(predictions_output.predictions, axis=1)\n",
    "\n",
    "# You can then compare these predicted_labels with predictions_output.label_ids\n",
    "# or perform further analysis as needed.\n",
    "print(\"Predicted Labels:\", predicted_labels)\n",
    "print(\"True Labels:\", predictions_output.label_ids)\n",
    "print(\"Evaluation Metrics:\", predictions_output.metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427115d-c2e4-424d-b21d-35db7b6cef44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
